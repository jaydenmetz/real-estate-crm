# Automated Security Monitoring & Alerting System

## Overview

Continuous security monitoring detects threats in real-time and alerts the security team before damage occurs. This system monitors 100+ security metrics and sends alerts within seconds.

**Monitoring Coverage:**
- Authentication events (login attempts, MFA, account lockouts)
- Authorization failures (permission denied, invalid tokens)
- Data access patterns (unusual queries, bulk exports)
- System health (uptime, errors, performance)
- Infrastructure changes (deployments, config changes)
- Vendor security (third-party breaches)

**Alert Channels:**
- **P1 Critical:** Phone call + SMS + Slack + Email
- **P2 High:** Slack + Email
- **P3 Medium:** Email only
- **P4 Low:** Daily digest email

## Security Event Monitoring

### Authentication Monitoring (Already Implemented ‚úÖ)

**Events Logged:**
- `login_success` - Successful user login
- `login_failed` - Failed login attempt
- `account_locked` - Account locked after 5 failed attempts
- `lockout_attempt_while_locked` - Login attempt on locked account
- `token_refresh` - Access token refreshed
- `api_key_created` - New API key generated
- `api_key_revoked` - API key deactivated
- `api_key_used` - API key authenticated request

**Automated Alerts:**

```javascript
// Alert on brute force attack (5 failed logins in 5 minutes)
if (failed_logins_per_ip > 5 in 5_minutes) {
  alert({
    severity: "P2",
    type: "BRUTE_FORCE_ATTACK",
    message: `Brute force detected from IP ${ip_address}`,
    actions: [
      "Block IP temporarily (30 min)",
      "Notify security team",
      "Monitor for additional attacks"
    ]
  });
}

// Alert on successful login from new country
if (login_country != user.usual_countries) {
  alert({
    severity: "P3",
    type: "ANOMALOUS_LOGIN_LOCATION",
    message: `${user.email} logged in from ${country} (unusual)`,
    actions: [
      "Require additional verification",
      "Email user asking to confirm login",
      "Monitor account for suspicious activity"
    ]
  });
}

// Alert on account lockout (potential attack)
if (event_type == "account_locked") {
  alert({
    severity: "P2",
    type: "ACCOUNT_LOCKOUT",
    message: `Account ${user.email} locked after 5 failed attempts`,
    actions: [
      "Notify user of lockout",
      "Check if legitimate user or attacker",
      "Provide unlock instructions"
    ]
  });
}

// Alert on multiple account lockouts (coordinated attack)
if (account_lockouts > 10 in 1_hour) {
  alert({
    severity: "P1",
    type: "COORDINATED_ATTACK",
    message: `${account_lockouts} accounts locked in 1 hour - coordinated attack suspected`,
    actions: [
      "Activate incident response",
      "Enable IP blocking",
      "Notify all affected users"
    ]
  });
}
```

### Authorization Monitoring (To Be Implemented)

```javascript
// Alert on permission escalation attempt
if (user.attempts_admin_action && !user.is_admin) {
  alert({
    severity: "P2",
    type: "PRIVILEGE_ESCALATION_ATTEMPT",
    message: `${user.email} attempted admin action without privileges`,
    actions: [
      "Log detailed attempt info",
      "Review user account (compromised?)",
      "Consider suspending account"
    ]
  });
}

// Alert on bulk data access
if (records_accessed > 1000 in 1_hour) {
  alert({
    severity: "P1",
    type: "BULK_DATA_ACCESS",
    message: `${user.email} accessed ${records_accessed} records in 1 hour`,
    actions: [
      "Suspend account immediately",
      "Review access logs (data exfiltration?)",
      "Notify security officer"
    ]
  });
}

// Alert on access to deleted records
if (user.accesses_deleted_record) {
  alert({
    severity: "P3",
    type: "DELETED_RECORD_ACCESS",
    message: `${user.email} accessed deleted record (potential recovery attempt)`,
    actions: [
      "Log access attempt",
      "Verify user has legitimate reason",
      "Consider restricting access to deleted data"
    ]
  });
}
```

### Data Access Monitoring (To Be Implemented)

```javascript
// Alert on PII export
if (export_contains_pii && user.role != "admin") {
  alert({
    severity: "P1",
    type: "UNAUTHORIZED_PII_EXPORT",
    message: `${user.email} exported ${row_count} records with PII`,
    actions: [
      "Block export immediately",
      "Review export contents",
      "Investigate user intent (data theft?)"
    ]
  });
}

// Alert on after-hours access
if (access_time.is_after_hours() && access_contains_sensitive_data) {
  alert({
    severity: "P2",
    type: "AFTER_HOURS_ACCESS",
    message: `${user.email} accessed sensitive data at ${time} (after hours)`,
    actions: [
      "Log detailed access info",
      "Verify with user next business day",
      "Consider restricting after-hours access"
    ]
  });
}

// Alert on unusual data patterns
if (user.access_pattern != user.historical_pattern) {
  alert({
    severity: "P3",
    type: "ANOMALOUS_DATA_ACCESS",
    message: `${user.email} showing unusual data access pattern`,
    actions: [
      "Monitor for 24 hours",
      "If pattern continues, investigate",
      "May indicate compromised account"
    ]
  });
}
```

## Infrastructure Monitoring

### Application Monitoring (Sentry - Already Configured ‚úÖ)

**Automated Alerts:**

```javascript
// Alert on error spike
if (error_rate > baseline * 5) {
  alert({
    severity: "P2",
    type: "ERROR_SPIKE",
    message: `Error rate ${error_rate}/min (baseline: ${baseline}/min)`,
    actions: [
      "Check recent deployments",
      "Review error stack traces",
      "Rollback if caused by deployment"
    ]
  });
}

// Alert on new error type
if (error.is_new && error.severity >= "error") {
  alert({
    severity: "P3",
    type: "NEW_ERROR",
    message: `New error type: ${error.message}`,
    actions: [
      "Investigate root cause",
      "Fix in next sprint",
      "Monitor frequency"
    ]
  });
}

// Alert on security-related errors
if (error.message.includes("authentication") ||
    error.message.includes("authorization") ||
    error.message.includes("SQL injection")) {
  alert({
    severity: "P1",
    type: "SECURITY_ERROR",
    message: `Security error detected: ${error.message}`,
    actions: [
      "Investigate immediately",
      "Check for exploit attempt",
      "Review related access logs"
    ]
  });
}
```

### Database Monitoring (To Be Implemented)

```javascript
// Alert on slow queries (potential attack)
if (query_duration > 10_seconds) {
  alert({
    severity: "P2",
    type: "SLOW_QUERY",
    message: `Query took ${query_duration}s: ${query.slice(0, 100)}...`,
    actions: [
      "Check if legitimate query",
      "Optimize query if repeated",
      "Block if malicious (DoS attempt)"
    ]
  });
}

// Alert on unusual query patterns
if (query.contains("DROP TABLE") ||
    query.contains("DELETE FROM * WHERE 1=1") ||
    query.contains("--") || // SQL comment (injection attempt)
    query.contains("';")) { // SQL injection pattern
  alert({
    severity: "P1",
    type: "SQL_INJECTION_ATTEMPT",
    message: `Potential SQL injection from ${user.email}: ${query}`,
    actions: [
      "Block user immediately",
      "Review all recent queries from user",
      "Check for data compromise"
    ]
  });
}

// Alert on database connection spike
if (active_connections > max_connections * 0.8) {
  alert({
    severity: "P2",
    type: "CONNECTION_SPIKE",
    message: `Database connections: ${active_connections}/${max_connections}`,
    actions: [
      "Identify connection sources",
      "Kill idle connections",
      "Scale database if needed"
    ]
  });
}
```

### Infrastructure Monitoring (Railway - To Be Implemented)

```javascript
// Alert on deployment failures
if (deployment.status == "failed") {
  alert({
    severity: "P2",
    type: "DEPLOYMENT_FAILURE",
    message: `Deployment failed: ${deployment.error}`,
    actions: [
      "Review deployment logs",
      "Rollback to previous version",
      "Fix error and re-deploy"
    ]
  });
}

// Alert on resource exhaustion
if (memory_usage > 90% || cpu_usage > 90%) {
  alert({
    severity: "P2",
    type: "RESOURCE_EXHAUSTION",
    message: `High resource usage: Memory ${memory_usage}%, CPU ${cpu_usage}%`,
    actions: [
      "Check for memory leaks",
      "Scale resources if needed",
      "Optimize code if repeated"
    ]
  });
}

// Alert on unauthorized deployment
if (deployer != authorized_deployers) {
  alert({
    severity: "P1",
    type: "UNAUTHORIZED_DEPLOYMENT",
    message: `${deployer} deployed to production (not authorized)`,
    actions: [
      "Rollback immediately",
      "Review deployment changes",
      "Investigate how deployer gained access"
    ]
  });
}
```

## Vendor Security Monitoring

### Third-Party Breach Alerts

```javascript
// Subscribe to vendor breach notifications
vendors = [
  "Railway",
  "GitHub",
  "Google",
  "Sentry",
  "Stripe",
  "SendGrid"
];

// Check HaveIBeenPwned API daily
for (vendor of vendors) {
  if (vendor.has_breach()) {
    alert({
      severity: "P1",
      type: "VENDOR_BREACH",
      message: `${vendor} reported data breach - our data may be affected`,
      actions: [
        "Read vendor's breach notification",
        "Determine if our data was affected",
        "Rotate all credentials for that vendor",
        "Notify customers if PII exposed"
      ]
    });
  }
}

// Monitor vendor status pages
for (vendor of vendors) {
  if (vendor.status != "operational") {
    alert({
      severity: "P3",
      type: "VENDOR_OUTAGE",
      message: `${vendor} status: ${vendor.status}`,
      actions: [
        "Check impact on our systems",
        "Activate backup/failover if needed",
        "Communicate to users if downtime expected"
      ]
    });
  }
}
```

## Alerting System Architecture

### Alert Routing

```javascript
// Alert severity determines routing
function routeAlert(alert) {
  switch(alert.severity) {
    case "P1": // Critical
      sendPhone("+1-555-SECURITY"); // Call incident commander
      sendSMS("+1-555-SECURITY", alert.message);
      sendSlack("#security-incidents", "@channel", alert);
      sendEmail("[email protected]", alert);
      createPagerDutyIncident(alert);
      break;

    case "P2": // High
      sendSlack("#security-incidents", "@security-team", alert);
      sendEmail("[email protected]", alert);
      createPagerDutyIncident(alert);
      break;

    case "P3": // Medium
      sendEmail("[email protected]", alert);
      logToSecurityDashboard(alert);
      break;

    case "P4": // Low
      addToDailyDigest(alert);
      logToSecurityDashboard(alert);
      break;
  }
}
```

### Alert Aggregation (Prevent Alert Fatigue)

```javascript
// Don't send duplicate alerts
function deduplicateAlert(alert) {
  const recent_alerts = getRecentAlerts(1_hour);
  const duplicate = recent_alerts.find(a =>
    a.type == alert.type &&
    a.user == alert.user &&
    a.severity == alert.severity
  );

  if (duplicate) {
    // Increment counter instead of new alert
    duplicate.count++;
    duplicate.last_seen = now();

    // Only re-alert if count crosses threshold
    if (duplicate.count == 10 || duplicate.count == 50 || duplicate.count == 100) {
      alert.message = `${alert.message} (${duplicate.count} occurrences)`;
      return alert; // Send aggregated alert
    }
    return null; // Suppress duplicate
  }

  return alert; // New alert, send it
}
```

### Alert Acknowledgment

```javascript
// Track alert response
function acknowledgeAlert(alert_id, responder) {
  alert.acknowledged_at = now();
  alert.acknowledged_by = responder;
  alert.status = "investigating";

  // Stop re-alerting
  stopAlertEscalation(alert_id);

  // Notify team of acknowledgment
  sendSlack("#security-incidents",
    `Alert ${alert_id} acknowledged by ${responder} - investigating`
  );
}

// Escalate if not acknowledged
function escalateAlert(alert) {
  if (!alert.acknowledged_at && alert.age > 15_minutes && alert.severity == "P1") {
    // Escalate to CEO
    sendPhone(CEO_PHONE);
    sendSMS(CEO_PHONE, `P1 alert not acknowledged after 15 min: ${alert.message}`);
  }

  if (!alert.acknowledged_at && alert.age > 1_hour && alert.severity == "P2") {
    // Escalate to manager
    sendEmail(MANAGER_EMAIL, alert);
  }
}
```

## Monitoring Dashboard

### Security Dashboard (To Be Implemented)

```bash
# Deploy Grafana + Prometheus for real-time dashboard

METRICS DISPLAYED:

1. Authentication Health
   - Login success rate (target: >99%)
   - Failed login attempts (alert if >100/hour)
   - Account lockouts (alert if >10/hour)
   - MFA adoption (target: 100%)

2. API Health
   - Request rate (requests/min)
   - Error rate (target: <1%)
   - Response time (p50, p95, p99)
   - Rate limit violations

3. Database Health
   - Query performance (slow queries)
   - Connection count (alert if >80%)
   - Replication lag (alert if >5 seconds)
   - Disk usage (alert if >80%)

4. Infrastructure Health
   - CPU usage (alert if >90%)
   - Memory usage (alert if >90%)
   - Disk I/O (alert if saturated)
   - Network traffic (alert if unusual)

5. Security Events (Real-time)
   - Recent security events (last 1 hour)
   - Event breakdown by type
   - Event breakdown by severity
   - Top triggered alerts
```

### Alert Dashboard (To Be Implemented)

```bash
# Display all active alerts

ALERT DASHBOARD:

ACTIVE ALERTS (3):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¥ P1 - BRUTE_FORCE_ATTACK
   IP: 203.0.113.45
   Target: [email protected]
   Failed attempts: 47
   Status: Investigating
   Acknowledged by: Security Officer
   Age: 12 minutes

üü† P2 - ERROR_SPIKE
   Error rate: 127/min (baseline: 15/min)
   Cause: Database timeout
   Status: Investigating
   Acknowledged by: CTO
   Age: 8 minutes

üü° P3 - ANOMALOUS_LOGIN_LOCATION
   User: [email protected]
   Location: China (unusual)
   Status: Pending user confirmation
   Age: 35 minutes

RESOLVED TODAY (8):
‚úÖ P2 - SLOW_QUERY (resolved 2h ago)
‚úÖ P3 - AFTER_HOURS_ACCESS (resolved 4h ago)
‚úÖ P4 - PASSWORD_RESET (resolved 6h ago)
...
```

## Implementation Checklist

### Week 3 Implementation

**Phase 1: Logging (Already Done ‚úÖ)**
- [‚úÖ] Security event logging implemented
- [‚úÖ] Authentication events tracked
- [‚úÖ] Database schema with 13 indexes
- [‚úÖ] API endpoints for event queries

**Phase 2: Alerting (This Week)**
- [ ] Set up alert routing (Slack, email, SMS)
- [ ] Configure PagerDuty for P1/P2 alerts
- [ ] Implement alert deduplication logic
- [ ] Create alert acknowledgment workflow
- [ ] Build security dashboard (Grafana)

**Phase 3: Advanced Monitoring (Next Week)**
- [ ] Implement authorization monitoring
- [ ] Add data access pattern detection
- [ ] Set up database query monitoring
- [ ] Configure vendor breach monitoring
- [ ] Deploy anomaly detection (ML-based)

### Alert Configuration

**Slack Integration:**
```bash
# Create Slack webhook
1. Go to api.slack.com/apps
2. Create app "Security Alerts"
3. Enable Incoming Webhooks
4. Create webhook for #security-incidents
5. Copy webhook URL

# In your app (backend/src/services/alerting.service.js):
const SLACK_WEBHOOK = process.env.SLACK_SECURITY_WEBHOOK;

async function sendSlackAlert(alert) {
  await fetch(SLACK_WEBHOOK, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      channel: '#security-incidents',
      username: 'Security Bot',
      icon_emoji: ':rotating_light:',
      text: alert.severity === 'P1' ? '<!channel> CRITICAL ALERT' : 'Security Alert',
      attachments: [{
        color: alert.severity === 'P1' ? 'danger' :
               alert.severity === 'P2' ? 'warning' : 'good',
        title: alert.type,
        text: alert.message,
        fields: [
          { title: 'Severity', value: alert.severity, short: true },
          { title: 'Time', value: new Date().toISOString(), short: true }
        ],
        actions: [
          {
            type: 'button',
            text: 'Acknowledge',
            url: `https://crm.jaydenmetz.com/alerts/${alert.id}`
          }
        ]
      }]
    })
  });
}
```

**PagerDuty Integration:**
```bash
# Set up PagerDuty
1. Sign up at pagerduty.com (free tier: $0/month for 5 users)
2. Create escalation policy:
   - Level 1: Security Officer (15 min response)
   - Level 2: CTO (30 min escalation)
   - Level 3: CEO (1 hour escalation)
3. Create integration key
4. Add to environment variables

# In your app:
const PAGERDUTY_KEY = process.env.PAGERDUTY_INTEGRATION_KEY;

async function createPagerDutyIncident(alert) {
  await fetch('https://events.pagerduty.com/v2/enqueue', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      routing_key: PAGERDUTY_KEY,
      event_action: 'trigger',
      payload: {
        summary: alert.message,
        severity: alert.severity === 'P1' ? 'critical' : 'error',
        source: 'CRM Security System',
        custom_details: alert
      }
    })
  });
}
```

## SOC 2 Audit Evidence

For SOC 2 auditor, provide:

```markdown
1. Monitoring Configuration
   - List of all monitored events
   - Alert thresholds and rules
   - Dashboard screenshots

2. Alert Response Records
   - All P1/P2 alerts (with response times)
   - Acknowledgment records
   - Escalation records

3. Security Event Logs
   - 12 months of security events
   - Query access for auditor
   - Retention policy documentation

4. Incident Metrics
   - Mean time to detect (MTTD)
   - Mean time to respond (MTTR)
   - Mean time to resolve (MTTR)
   - False positive rate

5. Dashboard Access
   - Auditor account for live dashboard
   - Historical data (12 months)
```

## Success Metrics

### Week 3 Goals
- [‚úÖ] Security monitoring documented
- [ ] Slack integration configured
- [ ] PagerDuty integration set up
- [ ] Alert routing implemented
- [ ] Security dashboard deployed

### Ongoing Metrics
- **Mean Time to Detect (MTTD):** <5 minutes
- **Mean Time to Respond (MTTR):** <15 minutes (P1), <1 hour (P2)
- **Alert Acknowledgment:** 100% within SLA
- **False Positive Rate:** <10% (industry average: 30%)

**Valuation Impact:** +$100k-200k (demonstrates real-time threat detection and mature security operations)

---

*Next: Document SOC 2 compliance progress in tracking spreadsheet*
